# üöÄ Guia Completo: PDF para Q&A no GCP do Zero
## üìã FASE 1: PREPARA√á√ÉO INICIAL
Pr√©-requisitos

Conta Google Cloud Platform
Cart√£o de cr√©dito cadastrado (para ativar recursos)
Google Cloud SDK instalado localmente

### 1.1 Configura√ß√£o Inicial do Projeto
bash# 1. Instalar Google Cloud SDK (se n√£o tiver)
### Windows: https://cloud.google.com/sdk/docs/install
### Mac: brew install google-cloud-sdk
### Linux: curl https://sdk.cloud.google.com | bash

## 2. Fazer login
gcloud auth login

## 3. Criar novo projeto (substitua por um ID √∫nico)
export PROJECT_ID="pdf-qa-generator-$(date +%s)"
gcloud projects create $PROJECT_ID --name="PDF QA Generator"

## 4. Configurar projeto como padr√£o
gcloud config set project $PROJECT_ID

## 5. Habilitar billing (OBRIGAT√ìRIO)
### V√° para: https://console.cloud.google.com/billing
### Associe seu projeto a uma conta de billing

## 6. Definir regi√£o padr√£o

export REGION="us-central1"
gcloud config set compute/region $REGION
1.2 Habilitar APIs Necess√°rias
bash# Habilitar todas as APIs necess√°rias (pode demorar alguns minutos)
echo "‚è≥ Habilitando APIs..."
gcloud services enable documentai.googleapis.com
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable storage.googleapis.com
gcloud services enable bigquery.googleapis.com
gcloud services enable aiplatform.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable eventarc.googleapis.com

echo "‚úÖ APIs habilitadas!"
üìÅ FASE 2: CRIA√á√ÉO DA INFRAESTRUTURA
2.1 Criar Buckets de Storage
bash# Criar buckets para PDFs de entrada e resultados
gsutil mb gs://${PROJECT_ID}-pdf-input
gsutil mb gs://${PROJECT_ID}-qa-output
gsutil mb gs://${PROJECT_ID}-function-source

echo "‚úÖ Buckets criados:"
echo "üì§ Input: gs://${PROJECT_ID}-pdf-input"
echo "üì• Output: gs://${PROJECT_ID}-qa-output"
2.2 Configurar Document AI
bash# Criar processador Document AI
echo "‚è≥ Criando Document AI Processor..."

# Via gcloud (m√©todo simples)
PROCESSOR_ID=$(gcloud ai document-processors create \
  --location=$REGION \
  --display-name="PDF-QA-Extractor" \
  --type="OCR_PROCESSOR" \
  --format="value(name)" | cut -d'/' -f6)

echo "‚úÖ Document AI Processor criado: $PROCESSOR_ID"
echo "üíæ Salvando PROCESSOR_ID..."
echo $PROCESSOR_ID > processor_id.txt
2.3 Criar Dataset BigQuery
bash# Criar dataset para armazenar resultados
bq mk --dataset --location=$REGION \
  --description="Dataset para Q&A pairs gerados" \
  ${PROJECT_ID}:qa_training_data

# Criar tabela
bq mk --table ${PROJECT_ID}:qa_training_data.generated_qa_pairs \
  instruction:STRING,output:STRING,content:STRING,source_file:STRING,chunk_id:INTEGER,created_at:TIMESTAMP

echo "‚úÖ BigQuery dataset e tabela criados!"
üíª FASE 3: DESENVOLVIMENTO DA CLOUD FUNCTION
3.1 Criar Estrutura de Arquivos
bash# Criar diret√≥rio do projeto
mkdir pdf-qa-generator
cd pdf-qa-generator

# Criar arquivos necess√°rios
touch main.py
touch requirements.txt
touch .env
3.2 Arquivo requirements.txt
txtgoogle-cloud-documentai==2.25.0
google-cloud-storage==2.13.0
google-cloud-bigquery==3.15.0
google-cloud-aiplatform==1.42.1
pandas==2.0.3
vertexai==1.42.1
functions-framework==3.5.0
3.3 Arquivo main.py (Cloud Function)
pythonimport os
import json
import pandas as pd
from typing import List, Dict
from datetime import datetime
import logging
import re

# Google Cloud imports
from google.cloud import storage
from google.cloud import documentai
from google.cloud import bigquery
import vertexai
from vertexai.preview.generative_models import GenerativeModel

# Configura√ß√µes
PROJECT_ID = os.environ.get('GCP_PROJECT')
LOCATION = os.environ.get('FUNCTION_REGION', 'us-central1')
PROCESSOR_ID = os.environ.get('PROCESSOR_ID')
OUTPUT_BUCKET = os.environ.get('OUTPUT_BUCKET')
BIGQUERY_DATASET = os.environ.get('BIGQUERY_DATASET', 'qa_training_data')

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def process_pdf_upload(cloud_event):
    """
    Cloud Function principal - triggered por upload de PDF
    """
    
    # Extrair informa√ß√µes do evento
    data = cloud_event.data
    bucket_name = data['bucket']
    file_name = data['name']
    
    logger.info(f"üìÑ Processando: {file_name} do bucket: {bucket_name}")
    
    # Verificar se √© PDF
    if not file_name.lower().endswith('.pdf'):
        logger.info(f"‚ùå Arquivo {file_name} n√£o √© PDF. Ignorando.")
        return 'OK'
    
    try:
        # 1. Extrair texto do PDF
        logger.info("üîç Extraindo texto com Document AI...")
        document_text = extract_text_from_pdf(bucket_name, file_name)
        
        if not document_text or len(document_text) < 100:
            logger.error(f"‚ùå Texto extra√≠do muito pequeno: {len(document_text)} chars")
            return 'ERROR'
        
        # 2. Dividir em chunks
        logger.info("‚úÇÔ∏è Dividindo documento em chunks...")
        chunks = chunk_document(document_text)
        logger.info(f"üìù Criados {len(chunks)} chunks")
        
        # 3. Gerar Q&A para cada chunk
        logger.info("ü§ñ Gerando Q&A com Gemini...")
        all_qa_pairs = []
        
        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) < 200:  # Skip chunks muito pequenos
                continue
                
            qa_pairs = generate_qa_from_chunk(chunk, i, file_name)
            all_qa_pairs.extend(qa_pairs)
            logger.info(f"‚úÖ Chunk {i}: {len(qa_pairs)} Q&A gerados")
        
        if not all_qa_pairs:
            logger.error("‚ùå Nenhum Q&A gerado!")
            return 'ERROR'
        
        # 4. Salvar resultados
        logger.info("üíæ Salvando resultados...")
        save_results(all_qa_pairs, file_name)
        
        logger.info(f"üéâ Processamento conclu√≠do! {len(all_qa_pairs)} Q&A pairs gerados")
        return 'OK'
        
    except Exception as e:
        logger.error(f"üí• Erro no processamento: {str(e)}")
        raise e

def extract_text_from_pdf(bucket_name: str, file_name: str) -> str:
    """Extrai texto usando Document AI"""
    
    try:
        # Cliente Document AI
        client = documentai.DocumentProcessorServiceClient()
        name = client.processor_path(PROJECT_ID, LOCATION, PROCESSOR_ID)
        
        # Baixar PDF do Storage
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(file_name)
        file_content = blob.download_as_bytes()
        
        # Processar documento
        raw_document = documentai.RawDocument(
            content=file_content,
            mime_type="application/pdf"
        )
        
        request = documentai.ProcessRequest(
            name=name,
            raw_document=raw_document
        )
        
        result = client.process_document(request=request)
        return result.document.text
        
    except Exception as e:
        logger.error(f"Erro na extra√ß√£o: {str(e)}")
        raise e

def chunk_document(text: str, chunk_size: int = 3000, overlap: int = 300) -> List[str]:
    """Divide documento em chunks com sobreposi√ß√£o"""
    
    chunks = []
    
    # Primeiro, dividir por se√ß√µes grandes (par√°grafos duplos)
    sections = re.split(r'\n\s*\n', text)
    
    current_chunk = ""
    
    for section in sections:
        if len(current_chunk) + len(section) < chunk_size:
            current_chunk += section + "\n\n"
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = section + "\n\n"
    
    # Adicionar √∫ltimo chunk
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def generate_qa_from_chunk(chunk: str, chunk_id: int, source_file: str) -> List[Dict]:
    """Gera Q&A usando Gemini"""
    
    try:
        # Inicializar Vertex AI
        vertexai.init(project=PROJECT_ID, location=LOCATION)
        model = GenerativeModel("gemini-1.5-pro")
        
        prompt = f"""
Voc√™ √© um especialista em criar datasets de treino para modelos de linguagem.

Com base no seguinte trecho de documento, crie EXATAMENTE 8 pares de pergunta-resposta que sejam:

1. ESPEC√çFICAS ao conte√∫do fornecido (n√£o gen√©ricas)
2. VARIADAS em tipo:
   - Defini√ß√µes e conceitos
   - Processos e procedimentos  
   - Causas e efeitos
   - Compara√ß√µes
   - An√°lises e interpreta√ß√µes
   - Dados espec√≠ficos e n√∫meros
3. CLARAS e bem formuladas
4. Com respostas COMPLETAS e PRECISAS baseadas no texto

IMPORTANTE: Responda APENAS com um JSON v√°lido:
[
    {{"instruction": "pergunta espec√≠fica 1", "output": "resposta completa 1"}},
    {{"instruction": "pergunta espec√≠fica 2", "output": "resposta completa 2"}},
    {{"instruction": "pergunta espec√≠fica 3", "output": "resposta completa 3"}},
    {{"instruction": "pergunta espec√≠fica 4", "output": "resposta completa 4"}},
    {{"instruction": "pergunta espec√≠fica 5", "output": "resposta completa 5"}},
    {{"instruction": "pergunta espec√≠fica 6", "output": "resposta completa 6"}},
    {{"instruction": "pergunta espec√≠fica 7", "output": "resposta completa 7"}},
    {{"instruction": "pergunta espec√≠fica 8", "output": "resposta completa 8"}}
]

Trecho do documento:
{chunk}
"""
        
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": 0.8,
                "top_p": 0.9,
                "max_output_tokens": 8000,
            }
        )
        
        # Limpar resposta
        response_text = response.text.strip()
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]
        
        # Parse JSON
        qa_pairs = json.loads(response_text)
        
        # Adicionar metadata
        for qa in qa_pairs:
            qa['chunk_id'] = chunk_id
            qa['source_file'] = source_file
            qa['content'] = chunk
            qa['created_at'] = datetime.utcnow().isoformat()
        
        return qa_pairs
        
    except json.JSONDecodeError as e:
        logger.error(f"Erro JSON para chunk {chunk_id}: {str(e)}")
        return []
    except Exception as e:
        logger.error(f"Erro Gemini para chunk {chunk_id}: {str(e)}")
        return []

def save_results(qa_pairs: List[Dict], source_file: str):
    """Salva no Storage e BigQuery"""
    
    # DataFrame
    df = pd.DataFrame(qa_pairs)
    
    # Nome do arquivo
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"qa_dataset_{source_file.replace('.pdf', '')}_{timestamp}.csv"
    
    # Salvar no Storage
    storage_client = storage.Client()
    bucket = storage_client.bucket(OUTPUT_BUCKET)
    blob = bucket.blob(f"datasets/{csv_filename}")
    
    csv_string = df.to_csv(index=False)
    blob.upload_from_string(csv_string, content_type='text/csv')
    
    logger.info(f"üíæ CSV salvo: gs://{OUTPUT_BUCKET}/datasets/{csv_filename}")
    
    # Salvar no BigQuery
    try:
        client = bigquery.Client(project=PROJECT_ID)
        table_id = f"{PROJECT_ID}.{BIGQUERY_DATASET}.generated_qa_pairs"
        
        job_config = bigquery.LoadJobConfig(
            write_disposition="WRITE_APPEND",
            autodetect=True
        )
        
        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
        job.result()
        
        logger.info(f"üìä Dados salvos no BigQuery: {table_id}")
        
    except Exception as e:
        logger.error(f"Erro BigQuery: {str(e)}")
üöÄ FASE 4: DEPLOY DA SOLU√á√ÉO
4.1 Deploy da Cloud Function
bash# Voltar para o diret√≥rio do projeto
cd pdf-qa-generator

# Obter PROCESSOR_ID salvo
PROCESSOR_ID=$(cat ../processor_id.txt)

# Deploy da fun√ß√£o
gcloud functions deploy pdf-qa-processor \
  --gen2 \
  --runtime=python311 \
  --region=$REGION \
  --source=. \
  --entry-point=process_pdf_upload \
  --trigger-bucket=${PROJECT_ID}-pdf-input \
  --memory=8GiB \
  --timeout=3600 \
  --max-instances=10 \
  --set-env-vars="PROCESSOR_ID=${PROCESSOR_ID},OUTPUT_BUCKET=${PROJECT_ID}-qa-output"

echo "‚úÖ Cloud Function deployada!"
echo "üéØ Trigger: gs://${PROJECT_ID}-pdf-input"
4.2 Configurar Permiss√µes
bash# Garantir permiss√µes corretas
PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format="value(projectNumber)")

# Permiss√£o para Vertex AI
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:${PROJECT_NUMBER}-compute@developer.gserviceaccount.com" \
  --role="roles/aiplatform.user"

# Permiss√£o para Document AI
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:${PROJECT_NUMBER}-compute@developer.gserviceaccount.com" \
  --role="roles/documentai.apiUser"

echo "‚úÖ Permiss√µes configuradas!"
üß™ FASE 5: TESTE DA SOLU√á√ÉO
5.1 Teste com PDF de Exemplo
bash# Baixar PDF de teste
curl -o teste.pdf "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"

# Fazer upload (isso vai triggerar a fun√ß√£o automaticamente)
gsutil cp teste.pdf gs://${PROJECT_ID}-pdf-input/

echo "üì§ PDF enviado! Aguarde o processamento..."
echo "üëÄ Acompanhe os logs:"
echo "gcloud functions logs read pdf-qa-processor --region=$REGION --limit=50"
5.2 Verificar Resultados
bash# Verificar arquivos gerados
echo "üìÅ Arquivos gerados:"
gsutil ls gs://${PROJECT_ID}-qa-output/datasets/

# Verificar dados no BigQuery
echo "üìä Dados no BigQuery:"
bq query --use_legacy_sql=false "
SELECT 
  source_file,
  COUNT(*) as qa_pairs,
  MIN(created_at) as first_generated,
  MAX(created_at) as last_generated
FROM \`${PROJECT_ID}.qa_training_data.generated_qa_pairs\`
GROUP BY source_file
"
üìä FASE 6: MONITORAMENTO E OTIMIZA√á√ÉO
6.1 Dashboard de Monitoramento
bash# Criar alertas
gcloud alpha monitoring policies create --policy-from-file=- <<EOF
{
  "displayName": "PDF QA Function Errors",
  "conditions": [
    {
      "displayName": "Function error rate",
      "conditionThreshold": {
        "filter": "resource.type=\"cloud_function\" AND resource.label.function_name=\"pdf-qa-processor\"",
        "comparison": "COMPARISON_GREATER_THAN",
        "thresholdValue": 0.1
      }
    }
  ]
}
EOF
6.2 Scripts de Manuten√ß√£o
bash# Script para limpeza autom√°tica (opcional)
cat > cleanup.sh << 'EOF'
#!/bin/bash
# Remove arquivos antigos (30+ dias)
gsutil -m rm gs://${PROJECT_ID}-pdf-input/**
gsutil lifecycle set - gs://${PROJECT_ID}-qa-output << LIFECYCLE
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "Delete"},
        "condition": {"age": 90}
      }
    ]
  }
}
LIFECYCLE
EOF

chmod +x cleanup.sh
üéâ SOLU√á√ÉO COMPLETA!
Como Usar:

Upload de PDF: gsutil cp seu_documento.pdf gs://${PROJECT_ID}-pdf-input/
Aguardar processamento: 2-10 minutos dependendo do tamanho
Baixar resultados: gsutil cp gs://${PROJECT_ID}-qa-output/datasets/*.csv ./

Monitoramento:

Logs: gcloud functions logs read pdf-qa-processor --region=$REGION
BigQuery: Console do BigQuery para an√°lises
Storage: Console do Cloud Storage para arquivos

Custos Estimados (por PDF de 100 p√°ginas):

Document AI: ~$1.50
Vertex AI (Gemini): ~$0.50
Cloud Functions: ~$0.10
Storage/BigQuery: ~$0.05
Total: ~$2.15 por documento
